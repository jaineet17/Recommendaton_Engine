version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: amazon-rec-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-amazon_rec_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis for Caching and Feature Store
  redis:
    image: redis:7-alpine
    container_name: amazon-rec-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
    restart: unless-stopped

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: amazon-rec-zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    ports:
      - "${ZOOKEEPER_PORT:-2181}:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo srvr | nc localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: amazon-rec-kafka
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT:-9092}:9092"
      - "${KAFKA_EXTERNAL_PORT:-29092}:29092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: unless-stopped

  # Kafka Initialize
  kafka-init:
    image: confluentinc/cp-kafka:7.3.0
    container_name: amazon-rec-kafka-init
    depends_on:
      - kafka
    volumes:
      - ./scripts/init-kafka.sh:/init-kafka.sh
    command: >
      bash -c "
        echo 'Waiting for Kafka to become available...' &&
        cub kafka-ready -b kafka:9092 1 300 &&
        chmod +x /init-kafka.sh &&
        /init-kafka.sh
      "
    restart: on-failure

  # MLflow for Model Registry
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: amazon-rec-mlflow
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-amazon_rec_db}
      --default-artifact-root s3://mlflow
    depends_on:
      - postgres
      - minio
    restart: unless-stopped

  # MinIO for artifact storage
  minio:
    image: minio/minio:RELEASE.2023-11-11T08-14-41Z
    container_name: amazon-rec-minio
    ports:
      - "${MINIO_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID:-minioadmin}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

  # MinIO Initialize
  minio-init:
    image: minio/mc:RELEASE.2023-11-11T08-40-00Z
    container_name: amazon-rec-minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        until mc config host add minio http://minio:9000 ${AWS_ACCESS_KEY_ID:-minioadmin} ${AWS_SECRET_ACCESS_KEY:-minioadmin}; do sleep 1; done;
        mc mb --ignore-existing minio/mlflow;
        mc policy set download minio/mlflow;
        exit 0;
      "
    restart: on-failure

  # API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: amazon-rec-api
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - FLASK_APP=src.api.app
      - FLASK_ENV=${FLASK_ENV:-production}
      - FLASK_DEBUG=0
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-amazon_rec_db}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SECRET_KEY=${SECRET_KEY:-change-me-in-production}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    volumes:
      - ./config:/app/config
      - ./data:/app/data
      - ./models:/app/models
    ports:
      - "${API_PORT:-5050}:5050"
    depends_on:
      - postgres
      - redis
      - kafka
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5050/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Candidate Generation Service
  candidate-generator:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: amazon-rec-candidate-generator
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICE_TYPE=candidate_generator
      - EMBEDDING_DIR=/app/data/embeddings
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    command: python -m src.services.candidate_generator
    volumes:
      - ./config:/app/config
      - ./data:/app/data
      - ./models:/app/models
    depends_on:
      - kafka
      - redis
      - mlflow
    restart: unless-stopped

  # Ranking Service
  ranker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: amazon-rec-ranker
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICE_TYPE=ranker
      - TRANSFORMER_MODEL_PATH=/app/models/transformer
      - ONNX_MODEL_PATH=/app/models/transformer.onnx
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    command: python -m src.services.transformer_ranker
    volumes:
      - ./config:/app/config
      - ./data:/app/data
      - ./models:/app/models
    depends_on:
      - kafka
      - redis
      - mlflow
      - candidate-generator
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ensemble Service
  ensemble:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: amazon-rec-ensemble
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICE_TYPE=ensemble
      - STACKING_MODEL_PATH=/app/models/stacking.pkl
      - MODEL_WEIGHTS_PATH=/app/config/model_weights.json
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    command: python -m src.services.hybrid_ensemble
    volumes:
      - ./config:/app/config
      - ./data:/app/data
      - ./models:/app/models
    depends_on:
      - kafka
      - redis
      - ranker
    restart: unless-stopped

  # Online Learning Service
  online-learning:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: amazon-rec-online-learning
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICE_TYPE=online_learning
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    command: python -m src.services.online_learning
    volumes:
      - ./config:/app/config
      - ./data:/app/data
      - ./models:/app/models
    depends_on:
      - kafka
      - redis
      - mlflow
    restart: unless-stopped

  # Feature Processor Service
  feature-processor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: amazon-rec-feature-processor
    environment:
      - CONFIG_PATH=/app/config/config.yaml
      - SERVICE_TYPE=feature_processor
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    command: python -m src.services.feature_processor
    volumes:
      - ./config:/app/config
      - ./data:/app/data
    depends_on:
      - kafka
      - redis
    restart: unless-stopped

  # Frontend
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: amazon-rec-frontend
    environment:
      - API_URL=http://api:5050
      - PORT=3000
    volumes:
      - ./src/frontend:/app/src
      - ./public:/app/public
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    depends_on:
      - api
    restart: unless-stopped

  # Prometheus for Metrics
  prometheus:
    image: prom/prometheus:v2.44.0
    container_name: amazon-rec-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # Grafana for Dashboards
  grafana:
    image: grafana/grafana:10.1.0
    container_name: amazon-rec-grafana
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    restart: unless-stopped

  # Jenkins for CI/CD
  jenkins:
    image: jenkins/jenkins:lts-jdk17
    container_name: amazon-rec-jenkins
    privileged: true
    user: root
    ports:
      - "${JENKINS_PORT:-8080}:8080"
      - "${JENKINS_AGENT_PORT:-50000}:50000"
    volumes:
      - jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - JAVA_OPTS=-Djenkins.install.runSetupWizard=false
    restart: unless-stopped

  # Elasticsearch for Logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4
    container_name: amazon-rec-elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "${ELASTICSEARCH_PORT:-9200}:9200"
    restart: unless-stopped

  # Kibana for Log Visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.4
    container_name: amazon-rec-kibana
    ports:
      - "${KIBANA_PORT:-5601}:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Filebeat for Log Collection
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.10.4
    container_name: amazon-rec-filebeat
    user: root
    volumes:
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
      - kibana
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  kafka_data:
  minio_data:
  prometheus_data:
  grafana_data:
  jenkins_home:
  elasticsearch_data: 